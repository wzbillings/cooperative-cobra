---
title: "Part 1: Tensors and Gradients"
date: last-modified
author: Zane Billings
toc: true
number-sections: true
---

OK, since we're doing Python now, the first thing we have to do is import
the packages we want to use (into the namespace, without attaching anything).
I actually strongly prefer Python's package
declaration system over R's (and I often use the `box` package for R to emulate
this behavior).

I was kind of confused that everyone calls this "PyTorch" but the library is
just called `torch`, but I guess that makes sense. Probably `torch` is
implemented in some low-level language and there are multiple high-level
interfaces to it. Maybe after this I should learn how to use `torch` with `R`.

```{python}
import torch
```

# Tensors

Apparently, `torch` is a high-performance framework for the manipulation of
tensors. I didn't know that until today.

The simplest way to think about a tensor is as a matrix that can have as
many different dimensions as you want. I think this doesn't really do them
justice in the same way that thinking about vectors as lists of numbers and
matrices as stacked vectors doesn't tell the whole truth, but tensors are hard
to think about. Anyways I'll stop rambling and just work through the examples
for a bit.

This code creates a tensor with one floating point element (AKA a zero-order
tensor or scalar).

```{python}
t1 = torch.tensor(4.0)
t1
```

We can check the type of the tensor.

```{python}
t1.dtype
```

It's interesting that the default is a 32-bit floating point but I guess
that is probably fine. I also noticed that you can make integer-valued
tensors, but not string-valued tensors.

Tensors of higher rank can be specified using Python's list syntax. (I guess
this is probably actually creating a list and then type casting it to a
tensor, but I'm not sure and I haven't looked into it.) Here we can make
use of some implicit conversion rules, which I believe are features of
`torch`. (As in Python, a list can have integers and floats in it at the same
time.) Apparently as long as we specify the first number as a float, we don't
have to worry about doing that for the rest of the elements.

It seems that `torch` tensors can only contain one type of element, similar to
a `matrix` in `R` (although those are allowed to contain characters).

```{python}
# Order 1 tensor (AKA vector)
t2 = torch.tensor([1., 2, 3, 4])
t2
t2.dtype
```

Specifying higher dimensional tensors is easily done using a list of lists.

```{python}
# Order 2 tensor (AKA matrix)
t3 = torch.tensor([[5., 6], [7, 8], [9, 10]])
t3

# Order 3 tensor (what I would casually call a tensor)
t4 = torch.tensor([
	[[11, 12, 13], [14, 15, 16]],
	[[17, 18, 19], [20, 21, 22.]]
])
t4
```

Note from this example that any of the elements is allowed to be floating point,
and the entire tensor will be filled with floats.

Like a matrix, a tensor cannot be "ragged" in any dimension. They must be
consistent within each dimension -- that is, if one row of a matrix has 7
elements, the rest of the rows must also have 7 elements. Tensors behave the
same way across all of their dimensions.

```{python}
#| error: TRUE
t5 = torch.tensor([[5., 6], [7, 8], [9]])
```


We can access the `.shape` attribute of a tensor to see the number of dimensions
and the length in each dimension.

```{python}
t1.shape
t2.shape
t3.shape
t4.shape
```

We can see that the scalar number, `t1`, has no dimensions. Other than that,
the dimensions are listed from "outermost" to "innermost" (if we think about
tensors as lists of lists of lists ofâ€¦).

# Basic tensor math

Standard arithmetic operations can be used on tensors as well. For example,
we can do scalar arithmetic. First we'll initialize the tensors we want.

```{python}
x = torch.tensor(3.)
w = torch.tensor(4., requires_grad = True)
b = torch.tensor(5., requires_grad = True)
x; w; b
```

Now we can create a new tensor with some math. We'll see what the
`requires_grad` option does in a moment.

```{python}
y = w * x + b
y
```

We check that
$$3(4) + 5 = 17$$
and see no surprises here. However! We are about to see the amazing autodiff
potential of `torch`. We call the `.backward()` method to find all of the
components of `y`, and compute the gradient of `y` with respect to those
components where `requires_grad = True`.

```{python}
y.backward()

print('dy/dx:', x.grad)
print('dy/dw:', w.grad)
print('dy/db:', b.grad)
```

The method of accessing these gradients is interesting to me -- I feel like
they should be properties of `y` rather than of the constituent parts, and
I also am not a fan of mutating methods, especially if it mutates an object
that I haven't even called explicitly. But I'm not here to complain about OOP
or the `torch` interface, I'm here to learn something new.

Anyways, we see that the derivatives are what we would expect. We get `None`
for the derivative wrt `x` because we didn't set `requires_grad = True` for `x`.
For `b` and `w` we get
$$\frac{dy}{db} = \frac{d}{db}\left(wx + b\right) = 0 + 1 = 1$$
and
$$\frac{dy}{dw} = \frac{d}{dw}\left(wx + b\right) = x + 0 = 3.$$

So as much as I don't like the syntax, that's pretty neat.

# Using `numpy` with `torch`

The `torch` package interoperates with `numpy`, which allows the use of
a lot of the `scipy` ecosystem, like `pandas` and `matplotlib`.

Here's how we can create an array in `numpy`.

```{python}
import numpy as np # This is conventional
x = np.array([[1, 2], [3, 4.]])
x
```

`torch` has a function for an explicit type conversion from `numpy`.

```{python}
y = torch.from_numpy(x)
y; x.dtype; y.dtype
```

Similarly, we can convert back to a `numpy` array.

```{python}
z = y.numpy
z
```

Now I remember all my frustrations of using python and trying to remember what
is a method and what is a mutator and what is a function. Functional programming
really is so nice and here I am forced back into the OOP world. But I suppose
I will get used to it over the course of this tutorial.

# Further reading

> 1. What if one or more of $x$, $w$, or $b$ were matrices, instead of
numbers in the above example? What would the result $y$ and the gradients
look like in this case?

It's been a while since I took linear algebra and vector calculus, so instead
of me trying to prove my answer and explain calculus on matrices, what if
I just type in some stuff and find out instead?

In this first case, $x$ is a vector, but $w$ and $b$ are scalars.

```{python}
x = torch.tensor([1, 2, 3, 4.])
y = x * w + b
y
```

So in this case, the operations were "broadcasted" over the vector $x$. That is,
each element of $x$ was multiplied by $w$, and then $b$ was added to each element.

I would bet that if $x$ were a matrix, we would see the same behavior.

```{python}
x = torch.tensor([[1, 2], [3, 4.]])
y = x * w + b
y
```

Yes, the same thing. That's not surprising, it's the most common way to define
these arithmetic operations between a higher-order tensor and a scalar. I
usually call this "elementwise" behavior but there are other names.

So what if we make $b$ a matrix?

```{python}
b = torch.tensor([[1, 2], [3, 4.]])
y = x * w + b
y
```

We get regular matrix addition, that's good. I assume that $b$ has to conform
in dimension or we'll get an error.

```{python}
#| error: true
b = torch.tensor([1, 2])
y = x * w + b
y
```

Hmm, no we don't. Recall that
$$x * w = \begin{bmatrix}4 & 8 \\ 12 & 16\end{bmatrix},$$
so it looks like $b$ was added to the first row and to the second row. That's
an interesting, and IMO understandard, broadcasting behavior. `R` sort of
behaves like this in some circumstances with vector recycling, but I would
strongly prefer to get an error about matrix conformation here. I can't think
of a circumstance when I'd want to do this where I wouldn't want to explicitly
turn $b$ into a matrix of the correct form.

Now maybe the most interesting test: what happens when $w$ is a vector or matrix?
For this part, I'll ignore $b$ and just do some tests.

```{python}
w = torch.tensor([2, 3])
x * w
```

```{python}
w = torch.tensor([[2, 3], [4, 5]])
x * w
```

Ok, so in both of these cases we see the same behavior. There is no attempt
to do a matrix or tensor product, so I assume we have special commands for
those. Instead we get the elementwise product with the same repeating
behavior that we saw before. What if we do something with really different
dimensions?

```{python}
#| error: TRUE
w = torch.tensor([[2, 3, 4], [4, 5, 6], [6, 7, 8]])
x * w
```

Ok, good, that doesn't work at all. I was worried part of $w$ would just get
cut off or something. What if only one of the dimensions is not-repeatable?

```{python}
#| error: TRUE
w = torch.tensor([[2, 3, 4], [4, 5, 6]])
x * w
```

Also an error, that's good.

```{python}
#| error: TRUE
w = torch.tensor([[2, 3], [4, 5], [6, 7], [8, 9]])
x * w
```

Ok interestingly, that time, $x$ doesn't get repeated. So I wonder if it
will only repeat the second tensor?

```{python}
w = x
x = torch.tensor([2, 3])
x * w
```

Nope, it will repeat the first one also, just only in some certain circumstances.

Ok, I could keep messing with this forever, but I should probably just read
the documentation about pytorch tensor math so I don't trick myself. Let's
move on.

> 2. What if $y$ were a matrix, with each element of the matrix expressed
as a combination of $x$, $w$, and $b$?

```{python}
x = torch.tensor(3.)
w = torch.tensor(4., requires_grad = True)
b = torch.tensor(5., requires_grad = True)
y = torch.tensor([w * x + b, b * x + w])
y
```

Ok, let's test this, then I'll test a $2\times 2$ version of $y$. Hopefully
`torch` will just use the regular rules of matrix differentiation.

```{python}
#| error: TRUE
y.backward()

print('dy/dx:', x.grad)
print('dy/dw:', w.grad)
print('dy/db:', b.grad)
```

Well, I have absolutely no idea what is going on here. Do we get the
same nothing for a matrix $y$?

```{python}
#| error: TRUE
y = torch.tensor(
	[[w * x + b, b * x + w], [w ** 2 * x, b ** 2 * x]],
	requires_grad = True
	)
y.backward()
```

Yes, I get the same runtime error that something is not working right. From the
error, I assume that this method won't actually let us calculate the derivative
of a matrix. Maybe I will learn more about that in this tutorial.

> 3. What if we had a chain of operations instead of just one? I.e.
$y = x * w + b; \quad z = l * y + m; \quad w = c * z + d$? What would calling
`w.grad` do?

I assume that we can use the chain rule to get these gradients (since I
assume backward means we are doing backpropagation, AKA, the chain rule).
Let's see if it works for the scalar tensors.

```{python}
c = torch.tensor(2., requires_grad = True)
z = torch.tensor(4., requires_grad = True)
d = torch.tensor(1./3., requires_grad = True)
x = torch.tensor(3.)
w = c * z + d
b = torch.tensor(5., requires_grad = True)
y = w * x + b
l = torch.tensor(0.5, requires_grad = True)
m = torch.tensor(0.3, requires_grad = True)
z = l * y + m
z.backward()
print("dz/dl:", l.grad)
print("dz/dm:", m.grad)
print("dz/dy:", y.grad)
```

So we can get the derivatives for $l$ and $m$, but not for $z$. I truly have
no idea what the rules of what I'm allowed to autodiff or not are, and the
documentation is not very understandable for beginners. So maybe I'll have
to find another tutorial to read.

<!-- END OF FILE -->
