---
title: "Part 1: Tensors and Gradients"
date: last-modified
author: Zane Billings
toc: true
number-sections: true
---

OK, since we're doing Python now, the first thing we have to do is import
the packages we want to use (into the namespace, without attaching anything).
I actually strongly prefer Python's package
declaration system over R's (and I often use the `box` package for R to emulate
this behavior).

I was kind of confused that everyone calls this "PyTorch" but the library is
just called `torch`, but I guess that makes sense. Probably `torch` is
implemented in some low-level language and there are multiple high-level
interfaces to it. Maybe after this I should learn how to use `torch` with `R`.

```{python}
import torch
```

# Tensors

Apparently, `torch` is a high-performance framework for the manipulation of
tensors. I didn't know that until today.

The simplest way to think about a tensor is as a matrix that can have as
many different dimensions as you want. I think this doesn't really do them
justice in the same way that thinking about vectors as lists of numbers and
matrices as stacked vectors doesn't tell the whole truth, but tensors are hard
to think about. Anyways I'll stop rambling and just work through the examples
for a bit.

This code creates a tensor with one floating point element (AKA a zero-order
tensor or scalar).

```{python}
t1 = torch.tensor(4.0)
t1
```

We can check the type of the tensor.

```{python}
t1.dtype
```

It's interesting that the default is a 32-bit floating point but I guess
that is probably fine. I also noticed that you can make integer-valued
tensors, but not string-valued tensors.

Tensors of higher rank can be specified using Python's list syntax. (I guess
this is probably actually creating a list and then type casting it to a
tensor, but I'm not sure and I haven't looked into it.) Here we can make
use of some implicit conversion rules, which I believe are features of
`torch`. (As in Python, a list can have integers and floats in it at the same
time.) Apparently as long as we specify the first number as a float, we don't
have to worry about doing that for the rest of the elements.

It seems that `torch` tensors can only contain one type of element, similar to
a `matrix` in `R` (although those are allowed to contain characters).

```{python}
# Order 1 tensor (AKA vector)
t2 = torch.tensor([1., 2, 3, 4])
t2
t2.dtype
```

Specifying higher dimensional tensors is easily done using a list of lists.

```{python}
# Order 2 tensor (AKA matrix)
t3 = torch.tensor([[5., 6], [7, 8], [9, 10]])
t3

# Order 3 tensor (what I would casually call a tensor)
t4 = torch.tensor([
	[[11, 12, 13], [14, 15, 16]],
	[[17, 18, 19], [20, 21, 22.]]
])
t4
```

Note from this example that any of the elements is allowed to be floating point,
and the entire tensor will be filled with floats.

Like a matrix, a tensor cannot be "ragged" in any dimension. They must be
consistent within each dimension -- that is, if one row of a matrix has 7
elements, the rest of the rows must also have 7 elements. Tensors behave the
same way across all of their dimensions.

```{python}
#| error: TRUE
t5 = torch.tensor([[5., 6], [7, 8], [9]])
```


We can access the `.shape` attribute of a tensor to see the number of dimensions
and the length in each dimension.

```{python}
t1.shape
t2.shape
t3.shape
t4.shape
```

We can see that the scalar number, `t1`, has no dimensions. Other than that,
the dimensions are listed from "outermost" to "innermost" (if we think about
tensors as lists of lists of lists ofâ€¦).

# Basic tensor math

Standard arithmetic operations can be used on tensors as well. For example,
we can do scalar arithmetic. First we'll initialize the tensors we want.

```{python}
x = torch.tensor(3.)
w = torch.tensor(4., requires_grad = True)
b = torch.tensor(5., requires_grad = True)
x; w; b
```

Now we can create a new tensor with some math. We'll see what the
`requires_grad` option does in a moment.

```{python}
y = w * x + b
y
```

We check that
$$3(4) + 5 = 17$$
and see no surprises here. However! We are about to see the amazing autodiff
potential of `torch`. We call the `.backward()` method to find all of the
components of `y`, and compute the gradient of `y` with respect to those
components where `requires_grad = True`.

```{python}
y.backward()

print('dy/dx:', x.grad)
print('dy/dw:', w.grad)
print('dy/db:', b.grad)
```

The method of accessing these gradients is interesting to me -- I feel like
they should be properties of `y` rather than of the constituent parts, and
I also am not a fan of mutating methods, especially if it mutates an object
that I haven't even called explicitly. But I'm not here to complain about OOP
or the `torch` interface, I'm here to learn something new.

Anyways, we see that the derivatives are what we would expect. We get `None`
for the derivative wrt `x` because we didn't set `requires_grad = True` for `x`.
For `b` and `w` we get
$$\frac{dy}{db} = \frac{d}{db}\left(wx + b\right) = 0 + 1 = 1$$
and
$$\frac{dy}{dw} = \frac{d}{dw}\left(wx + b\right) = x + 0 = 3.$$

So as much as I don't like the syntax, that's pretty neat.

# Using `numpy` with `torch`

The `torch` package interoperates with `numpy`, which allows the use of
a lot of the `scipy` ecosystem, like `pandas` and `matplotlib`.

Here's how we can create an array in `numpy`.

```{python}
import numpy as np # This is conventional
x = np.array([[1, 2], [3, 4.]])
x
```

`torch` has a function for an explicit type conversion from `numpy`.

```{python}
y = torch.from_numpy(x)
y; x.dtype; y.dtype
```

Similarly, we can convert back to a `numpy` array.

```{python}
z = y.numpy
z
```

Now I remember all my frustrations of using python and trying to remember what
is a method and what is a mutator and what is a function. Functional programming
really is so nice and here I am forced back into the OOP world. But I suppose
I will get used to it over the course of this tutorial.

<!-- END OF FILE -->
