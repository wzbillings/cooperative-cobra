---
title: "Part 3: Logistic Regression and Working with Images"
date: last-modified
author: Zane Billings
toc: true
number-sections: true
---

This section will be about "logistic" regression (see why I don't like
the name later) as well as working with images in `torch` using the
MNIST dataset. Working with the MNIST dataset is something I read a lot
of examples about but nothing something. First we need to get the
dataset.

```{python}
import torch
import torchvision

# Download the data
train = torchvision.datasets.MNIST(root = "static/", download = True)
test = torchvision.datasets.MNIST(root = "static/", train = False)
```

Now since the dataset is composed of 28x28 images, we can plot one of
them to see what we are dealing with.

```{python}
import matplotlib.pyplot as plt
ex_plot = plt.imshow(train[0][0], cmap = 'gray')
plt.show(ex_plot)
```

So now that we have all these images (which we will load lazily, which I
learned is actually a nice property of a tensorflow dataset / data
loader) we need to turn them into something we can use and model with
`torch`. So first we have to convert the images into tensors. Apparently
the `torchvision.transforms` has specific functionality for doing this.

```{python}
import torchvision.transforms
train = torchvision.datasets.MNIST(
	root = "static/",
	train = True,
	transform = torchvision.transforms.ToTensor()
)
test = torchvision.datasets.MNIST(
	root = "static/",
	train = False,
	transform = torchvision.transforms.ToTensor()
)
```

I already know how a grayscale image is stored in the computer (thanks
to when I had to do a bunch of microscopy stuff in undergrad) so I'll
skip messing around with that. The resulting tensor will be 1x28x28
(instead of just 28x28 for whatever reason, idk) where each of the
values is the intensity of that pixel. Anyways now that we have actual
tensors that we can use in `torch` models, we want to do a bit more
housekeeping.

`R` guru Julia Silge calls this bit "spending the data budget" which I
like and I've taken to saying that also. We already have the standard
test set (used so that different researchers can directly compare their
models on this canonical dataset) but we'll also want a validation set
so we can do better at training out model. We'll take a set of the
training data and use it as the validation set. We're allowed to test
our models' performances on this dataset, unlike the test set which we
cannot see until we have finished model-building if we want unbiased
performance metrics.

```{python}
train_ds, val_ds = torch.utils.data.random_split(
	train, [50000, 10000]
)
```

And since this dataset is so big and also filled with images, we'll want
to create a data loader to handle batch loading the data. I think this
is actually a good application of this, unlike last time. Note that I
picked 128 as the batch size just cause the tutorial did. We want to
shuffle the training data each time it is loaded because this can help
in training the model, but since the validation data are only used for
evaluation, not for model fitting, they do not need to be shuffled (it
doesn't matter if they are or not).

```{python}
from torch.utils.data import DataLoader
bs = 128
train_loader = DataLoader(train_ds, bs, shuffle = True)
val_loader = DataLoader(test, bs)
```

# Doing "logistic" regression

Now this is the one part of the tutorial that I take issue with. It
keeps calling this *logistic regression*. What we're actually doing here
is called *softmax multinomial regression*. According to wikipedia, a
lot of people also call it logistic regression and it's a common way to
refer to this model. But the machine learning people who call it that
also use stochastic gradient descent for regression, so. You know.
Anyways, logistic regression gets its name from the logit link function,
which we aren't using here. So I don't think it's appropriate to call
this model logistic regression. Technically we shouldn't even call it
softmax regression, we should call it softargmax regression, because
softmax is an approximation to the argmax function, not to the max
function. So you know what? I'm going to call it *softargmax multinomial
regression* and none of you can stop me. Let's call things what they
actually are.

Anyways.

We will, of course, begin by constructing a neural network with a neural
network with 10 target nodes (for the digits 0 to 9 in the data set).
Since we're doing torch or whatever we have to do a neural net for
everything. And since we're doing softargmax multinomial regression, we
get 10 output classes instead of the standard 9 plus one reference
group. Which is fine, I don't have a problem with that part. We've been
through this before so I won't say too much more about it.

The first argument is the input space. We actually have 28 x 28 features
per image, that's how many pixels we have and each one is a feature
we'll use for model fitting. Then we have the 10 classes that will be
the number of target nodes we have.

```{python}
import torch.nn as nn

model = nn.Linear(28 * 28, 10)
```

This has *a lot* more parameters than any of the examples we've seen
before, but it's actually the same thing. It's like if you had a little
clock, it's basically the same as a big clock but just different sizes.
If you know how the little one works you pretty much know how the big
one works. (Clockmakers DO NOT call me out.)

Now one thing we will have to deal with is the dimensions of our images.
We've just told our model that we have $28 \times 28 = 784$ input
variables. So of course it will complain if we put in a $28 \times 28$
tensor instead of a $1 \times 784$ vector tensor of inputs in the
correct order, it just won't know what to do with all of them. And we
gotta make sure everything stays in the right order or our model will
just be absolutely donked.

For some ungodly reason, the tutorial wants me to define a new class
here. I'm sure there's going to be an example later on where this
actually makes sense. But right now, defining a new class just to call
the `.reshape` method on the tensors is INSANE to me. Like using
gradient descent for linear regression we are again trying to fill a
thimble with a fire hose.

I actually tried to do it a bunch of different ways but due to the way
everything in python is an insane and there's no consistency to what's
immutable and what things get returned from functions, I'll just do it
this way. Even though it makes much more sense to any rational human to
just reshape all of the data first. Especially if we're fitting all the
models multiple times. That's just like, good programming (not repeating
code that doesn't need to be repeated). I think what I have below is not
the *best* way to do this, but it's the only way I could find that
didn't require me to load everything into memory at once, since I
couldn't figure out how to overwrite each item from a DataSet as it was
loaded.

I hate repeating this much code, but if I have to do it to avoid
needlessly repeating a transformation every time I call the model, I
will. I guess the age of code optimization is truly dead because
apparently (at least from my naive googling) there is no simple way to
apply a transformation to an entire DataSet after it is created. This
entire `torch` ecosystem is insane to me, maybe I need to learn
`tensorflow` also and see if it makes me want to smack my head into the
wall the same amount or not.

After reshaping them, I also invoked the `squeeze()` method to ensure that
the outputs don't have an extra useless dimension.

```{python}
train_val = torchvision.datasets.MNIST(
	root = "static/",
	train = True,
	transform = torchvision.transforms.Compose([
		torchvision.transforms.ToTensor(),
		torchvision.transforms.Lambda(lambda x: x.reshape(-1, 784).squeeze())
		])
)

test_flat = torchvision.datasets.MNIST(
	root = "static/",
	train = False,
	transform = torchvision.transforms.Compose([
	torchvision.transforms.ToTensor(),
	torchvision.transforms.Lambda(lambda x: x.reshape(-1, 784).squeeze())
	])
)

train_flat, val_flat = torch.utils.data.random_split(
	train_val, [50000, 10000]
)

train_loader = DataLoader(train_flat, bs, shuffle = True)
val_loader = DataLoader(val_flat, bs)
```

But I guess who cares if our code is good or optimized if we're doing
deep learning, right? Anyways now let's check that our model works.

```{python}
for img, lbl in train_loader:
	print("Original size:", img.shape)
	out = model(img.squeeze())
	print(out)
	break
```

Yes, our model runs now, thank goodness. If it didn't I might call it
quits and decided that this pytorch nonsense isn't for me. But now I
shall persist, I suppose.

Now we can fit the model. We'll use `softargmax` (aka the erroneously
named softmax) as the "activation function". Since we only have one
layer in this neural net (since it is a linear model), this basically
means we will take the inputs, put them in the linear model, then put
them through the softargmax function.

```{python}
# Rename it to the good name :)
def softargmax(input, dim = None, _stacklevel = 3, dtype = None):
	return torch.nn.functional.softmax(input, dim, _stacklevel, dtype)

softargmax(out, dim = 1)
```

OK, it looks like everything is working. For every image, we get 10 outputs
where each one is the predicted probability that our image is a number 0, 1,
â€¦, 9. We can check that the output probabilies for a given image add up to 1.

```{python}
torch.sum(softargmax(out, dim = 1)[0]).item()
```

Now we need to set up the remaining components of the modeling setup: the
loss function and the optimizer. I'll use the tutorial's suggestion of the
cross-entropy.

```{python}
loss_fcn = torch.nn.functional.cross_entropy
loss = loss_fcn(out, lbl)
print("Cross-entropy:", round(loss.item(), 4))
print(
	"Avg predicted probability of correct label:",
	round(torch.exp(-loss).item(), 4)
)
```

OK, so we can see that our random guess of weights and biases isn't very good.
The average predicted probability of the correct label is quite small. But
never fear, the next stem is to train our model. For the optimizer,
we'll use stochastic gradient descent. We'll also run for a fit number of
epochs.

For whatever reason the tutorial decided to include undefined methods
in this function and then randomly say "btw let's add more methods to the
custom model class" which is still just an insane idea to me. So I kind of
rewrote this, but it does the same thing. Just without an insane
custom class that introduces a crap ton of overhead repeated calculations. Like
I guess it looks fancier if you just define a bunch of custom stuff that
you don't really need, but it's not like we're repeating a bunch of code that
we need to dry out or something, it's just kind of pointless to me to do it
that way.

```{python}
def fit(epochs, learn_rate, opt_fcn = torch.optim.SGD, verbose = True, tol = 1e-6):
	optimizer = opt_fcn(model.parameters(), learn_rate)
	# Container for recording the results in each epoch. I decided to
	# preallocate the list space because it definitely saves time, I'm just
	# not sure how much.
	history = [0] * epochs
	
	for i in range(epochs):
		# Train the model
		for dat, lbl in train_loader:
			pred = model(dat)
			loss = loss_fcn(pred, lbl)
			loss.backward()
			optimizer.step()
			optimizer.zero_grad()
		
		# Holder for validation performance during this epoch
		this_batch = [0] * len(val_loader)
		j = 1
		
		# Evaluate the model
		for dat, lbl in val_loader:
			res = model(dat)
			this_batch[j] = loss_fcn(res, lbl).item()
			j += 1
			break
		
		history[i] = sum(this_batch) / len(this_batch)
		
		# Print every 100th epoch
		if ((i + 1) % 100 == 0) and verbose:
			print(
				'Epoch [{}/{}], Loss: {:.4f}'.format(
					i + 1,
					epochs,
					history[i]
					)
				)
		
		# Check for early stopping criteria
		if (i > 1) and (abs(history[i] - history[i - 1]) <= tol):
			print(
				"Loss converged, with tolerance {} at epoch {}\n".format(
					abs(history[i] - history[i - 1]),
					i + 1
					),
				"Stopping early."
			)
			break
	
	return history
```

Now we'll fit the model.

```{python}
hist = fit(10000, 1e-3)
```

Ok, so the model fitting converged and stopped early which is good.

```{python}
import numpy as np
plt.clf()
plt.plot(
	list(range(hist.index(0))),
	np.exp([-x for x in hist[0:hist.index(0)]])
)
plt.xlabel("epoch")
plt.ylabel("pred. prob of true class")
plt.show()
```

```{python}

```


<!-- END OF FILE -->
