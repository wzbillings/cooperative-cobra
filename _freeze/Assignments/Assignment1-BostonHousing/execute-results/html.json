{
  "hash": "55e51ed7df0578a781bea069b3c9a91d",
  "result": {
    "markdown": "---\ntitle: \"Assignment 1: Boston Housing regression model\"\ndate: last-modified\nauthor: Zane Billings\ntoc: true\nnumber-sections: true\n---\n\nThe original assignment for this section was to find 5 functions in the\nPytorch documentation, write about them with examples, and write a Medium\npost about it `r emoji::emoji(\"eyeroll\")`. No wonder there's so many\nbasically useless Medium articles in existence if people like me who have\nabsolutely no idea what they're doing are basically creating worse versions\nof all the documentation and good examples that already exist. So I decided\nto make my own assignment.\n\nI was going to download the dataset from\n[this kaggle competition](https://www.kaggle.com/competitions/boston-housing/overview)\nlinked in the tutorial, but it won't let me download the dataset, of course.\nSo, I did what any reasonable person would do and found a separate link to\nthe dataset on kaggle, and uploaded it to\n[my GitHub](https://github.com/wzbillings/datasets/tree/main/BostonHousing)\nso I don't have to deal with Kaggle's terrible website or worry about the\ndata link randomly going away. So feel free to play along at home, if you want.\nAlthough idk why you would be reading my insane notes instead of just watching\nthe same tutorial that I watched.\n\nBut regardless, here's the raw link to the CSV to save you some time:\n[link](https://raw.githubusercontent.com/wzbillings/datasets/main/BostonHousing/HousingData.csv).\n\n# Data Import\n\nOk, so the first thing I have to do is figure out how to load a CSV from a URL\ninto python. Then I have to figure out how to convert that into a `torch`\ntensor object. Luckily, I have a lot of patience for this kind of thing. I think\nI will need `pandas` for this, so I went ahead and installed it.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\n# Read the CSV direct from GitHub\nurl = \"\".join([\n\t\t\"https://raw.githubusercontent.com/wzbillings/datasets/main/\",\n\t\t\"BostonHousing/HousingData.csv\"\n\t])\n\nhousing = pd.read_csv(url)\n```\n:::\n\n\nOK, before I convert to Tensors, I think I need to get rid of the missing data.\nI'll do a quick and dirty KNN imputation with 5 neighbors, which is apparently\nthe default in sklearn. Idk how to tune it to choose a better one with this.\nThat's something to look into in the future.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.impute import KNNImputer\n\n# Create the imputer\nimputer = KNNImputer(n_neighbors = 5, weights = \"uniform\")\nimputed_df = pd.DataFrame(imputer.fit_transform(housing))\nimputed_df.columns = housing.columns\nimputed_df.index = housing.index\n```\n:::\n\n\nOK, now I think we can convert to tensor. It kind of annoys me that there is\napparently no built-in interoperability between sklearn and pandas.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Convert to TensorDataset\nimport torch\nfrom torch.utils.data import TensorDataset\n\n# Convert from pandas DataFrame to torch tensor\ninputs = torch.tensor(imputed_df.drop(\"MEDV\", axis = 1).values)\ntargets = torch.tensor(\n\tnp.reshape(imputed_df[\"MEDV\"].values, (-1, 1)),\n\t# Make sure both tensors are the same type\n\tdtype = inputs.dtype\n)\n```\n:::\n\n\nOk, that only took a few google searches so I guess I remember more about\n`pandas` than I thought. Since there are only `r 506 * 14` entries in the\ndata frame, I'm not going to worry about batch loading or whatever.\n\nOf course my first instinct as a data scientist is to plot the data,\nbut this course is about tensorflow, not about data visualization in python --\nthat's another course that I need to take. I started working on this before\nrealizing that you can't show a nice 14x14 scatterplot grid, and then I tried\nto slice the dataframe to get the first six columns PLUS the outcome variable.\nBut of course this is apparently a very difficult task in python, even though\nit would be `housing[,c(0:6, 14)]` in R. But I found multiple stackoverflow\nthreads talking about how to do this, and apparently the simplest solution is\n`housing[housing.columns[[x for x in range(7)] + [14]]]` which is INSANE to me.\n\nBut I'll stop complaining about python again, not being\nable to easily make a scatterplot is *fine* I guess.\n\nSo, the first thing we need to do is set up the model, loss function, and\noptimizer. I'll use the linear neural net and MSE loss just like in the\ntutorial, since that gives us a standard linear regression model. I looked\nat the options, and one of them is L-BFGS, which is one of the defaults in `R`\nand I quite like it. So let's use that instead of stochastic gradient descent.\nIt's not a stochastic method, but since we're guaranteed to have only one\nextremum with this problem, it will be good.\n\nI'm not really sure what the best way to choose the learning rate is, so I'll\nuse the same one from the previous example.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nmodel = torch.nn.Linear(housing.shape[1]-1, 1, dtype = torch.float64)\nloss_fn = torch.nn.functional.mse_loss\noptim = torch.optim.LBFGS(model.parameters(), lr = 1e-5)\n```\n:::\n\n\nFrom the [documentation](https://pytorch.org/docs/stable/optim.html#optimizer-step-closure),\napparently LBFGS needs a \"closure\" argument to\nrevaluate the model multiple times, so we have to define that by following\nthe example.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef optim_closure():\n\toptim.zero_grad()\n\toutput = model(inputs)\n\tloss = loss_fn(output, targets)\n\tloss.backward()\n\treturn loss\n```\n:::\n\n\nNow we will write our function for fitting the model. I'll go back to\nwaiting for the error to converge this time, since I think in general that\nis a better approach than running for a fixed number of epochs. But we\ncan also add a maximum iteration number to terminate at.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef fit_model(max_epochs):\n\t\"\"\" This puts it all together.\"\"\"\n\t# Initiate some constants that will be used in the loop\n\tepochs_run = 0\n\tloss_diff = 1\n\tloss = torch.tensor(1)\n\t\n\twhile ((epochs_run <= max_epochs) and (loss_diff > 1e-4)):\n\t\t# Save the old loss so we can calculate the difference\n\t\told_loss = loss\n\t\t# Fit the model with current parameters\n\t\tpred = model(inputs)\n\t\t# Calculate the loss\n\t\tloss = loss_fn(pred, targets)\n\t\t# Compute the gradients\n\t\tloss.backward()\n\t\t# Update the parameters with the closure step\n\t\toptim.step(optim_closure)\n\t\t# Reset the gradients\n\t\toptim.zero_grad()\n\n\t\t# Calculate the difference in the loss function\n\t\tloss_diff = abs(loss - old_loss)\n\n\t\t# Increment epochs\n\t\tepochs_run += 1\n\t\t\n\t\t# Print current progress every 100th epoch\n\t\tif (epochs_run) % 100 == 0:\n\t\t\tprint(\n\t\t\t\t'Epoch [{}/{}], Loss: {:.4f}'.format(\n\t\t\t\t\tepochs_run,\n\t\t\t\t\tmax_epochs,\n\t\t\t\t\tloss.item()\n\t\t\t\t),\n\t\t\t\tflush = True\n\t\t\t)\n\t\n\t# If we reached maximum number of epochs before loss converged, report that\n\tif epochs_run == max_epochs:\n\t\tprint(\"Maximum epochs reached without convergence.\", flush = True)\n```\n:::\n\n\nNow we run the stuff. It should take **all** that long, so I'm willing to\nrun it for like 10000 iterations while I get up from the computer and walk\naround. Just remember while this is running, this model would probably\ntake less than a second to fit with a regular linear solution solver.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfit_model(1e4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [100/10000.0], Loss: 621.1790\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [200/10000.0], Loss: 598.6065\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [300/10000.0], Loss: 576.8173\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [400/10000.0], Loss: 555.8203\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [500/10000.0], Loss: 535.5941\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [600/10000.0], Loss: 516.1251\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [700/10000.0], Loss: 497.3804\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [800/10000.0], Loss: 479.3326\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [900/10000.0], Loss: 461.9538\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1000/10000.0], Loss: 445.2149\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1100/10000.0], Loss: 429.0900\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1200/10000.0], Loss: 413.5533\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1300/10000.0], Loss: 398.5789\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1400/10000.0], Loss: 384.1466\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1500/10000.0], Loss: 370.2379\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1600/10000.0], Loss: 356.8259\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1700/10000.0], Loss: 343.8998\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1800/10000.0], Loss: 331.4394\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [1900/10000.0], Loss: 319.4383\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2000/10000.0], Loss: 307.8878\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2100/10000.0], Loss: 296.7706\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2200/10000.0], Loss: 286.0787\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2300/10000.0], Loss: 275.7921\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2400/10000.0], Loss: 265.8972\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2500/10000.0], Loss: 256.3795\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2600/10000.0], Loss: 247.2260\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2700/10000.0], Loss: 238.4395\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2800/10000.0], Loss: 230.0111\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [2900/10000.0], Loss: 221.9286\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3000/10000.0], Loss: 214.1719\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3100/10000.0], Loss: 206.7073\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3200/10000.0], Loss: 199.5383\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3300/10000.0], Loss: 192.6485\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3400/10000.0], Loss: 186.0289\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3500/10000.0], Loss: 179.6617\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3600/10000.0], Loss: 173.5416\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3700/10000.0], Loss: 167.6569\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3800/10000.0], Loss: 162.0026\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [3900/10000.0], Loss: 156.5712\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4000/10000.0], Loss: 151.3546\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4100/10000.0], Loss: 146.3447\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4200/10000.0], Loss: 141.5333\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4300/10000.0], Loss: 136.9139\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4400/10000.0], Loss: 132.4752\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4500/10000.0], Loss: 128.2115\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4600/10000.0], Loss: 124.1157\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4700/10000.0], Loss: 120.1809\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4800/10000.0], Loss: 116.4008\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [4900/10000.0], Loss: 112.7692\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5000/10000.0], Loss: 109.2819\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5100/10000.0], Loss: 105.9304\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5200/10000.0], Loss: 102.7108\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5300/10000.0], Loss: 99.6181\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5400/10000.0], Loss: 96.6474\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5500/10000.0], Loss: 93.7939\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5600/10000.0], Loss: 91.0532\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5700/10000.0], Loss: 88.4219\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5800/10000.0], Loss: 85.8935\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [5900/10000.0], Loss: 83.4652\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6000/10000.0], Loss: 81.1330\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6100/10000.0], Loss: 78.8931\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6200/10000.0], Loss: 76.7419\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6300/10000.0], Loss: 74.6760\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6400/10000.0], Loss: 72.6949\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6500/10000.0], Loss: 70.7923\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6600/10000.0], Loss: 68.9644\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6700/10000.0], Loss: 67.2093\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6800/10000.0], Loss: 65.5223\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [6900/10000.0], Loss: 63.9038\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7000/10000.0], Loss: 62.3479\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7100/10000.0], Loss: 60.8535\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7200/10000.0], Loss: 59.4184\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7300/10000.0], Loss: 58.0406\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7400/10000.0], Loss: 56.7159\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7500/10000.0], Loss: 55.4448\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7600/10000.0], Loss: 54.2233\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7700/10000.0], Loss: 53.0499\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7800/10000.0], Loss: 51.9212\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [7900/10000.0], Loss: 50.8375\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8000/10000.0], Loss: 49.7965\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8100/10000.0], Loss: 48.7974\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8200/10000.0], Loss: 47.8372\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8300/10000.0], Loss: 46.9147\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8400/10000.0], Loss: 46.0289\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8500/10000.0], Loss: 45.1777\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8600/10000.0], Loss: 44.3598\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8700/10000.0], Loss: 43.5745\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8800/10000.0], Loss: 42.8198\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [8900/10000.0], Loss: 42.0953\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9000/10000.0], Loss: 41.3988\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9100/10000.0], Loss: 40.7295\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9200/10000.0], Loss: 40.0869\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9300/10000.0], Loss: 39.4693\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9400/10000.0], Loss: 38.8761\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9500/10000.0], Loss: 38.3062\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9600/10000.0], Loss: 37.7580\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9700/10000.0], Loss: 37.2317\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9800/10000.0], Loss: 36.7256\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [9900/10000.0], Loss: 36.2385\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch [10000/10000.0], Loss: 35.7703\n```\n:::\n:::\n\n\nOK, so we got down to a reasonable loss. I'm pretty sure we could get a better\nsolution if we ran for longer but let's take a look at the parameters now.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nlist(model.parameters())\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[Parameter containing:\n tensor([[-0.0804,  0.0209,  0.0188,  0.7246,  0.2983,  4.7973, -0.0196, -0.8471,\n           0.1883, -0.0237, -0.2906,  0.0376, -0.3848]], dtype=torch.float64,\n        requires_grad=True),\n Parameter containing:\n tensor([0.4049], dtype=torch.float64, requires_grad=True)]\n```\n:::\n:::\n\n\nIt's important to remember that the first entry here is the vector of\nslope coefficients, and the bias is the intercept. We can compare this to\na regular regression model. I'll even do it by hand just to show how much\nwe should not be doing linear regression this way using tensors and a numerical\noptimizer.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Create a vector of ones -- these are used to estimate the \"bias\".\nones = torch.tensor(np.ones(shape = [506, 1]))\n# Add the vector of ones as the first column of the imputed input tensor.\ni2 = torch.cat((ones, inputs), dim = -1)\n\n# Calculate the analytical solution: (X^T X)^-1 X^T y.\nsoln = torch.linalg.inv(i2.t() @ i2) @ i2.t() @ targets\n\n# Get the predictions: just X beta_hat\nsoln_preds = i2 @ soln\n\n# And now calculate the MSE for the analytical solution vs. the one that\n# ran for 10000 iterations.\nsoln_mse = loss_fn(soln_preds, targets)\noptim_mse = loss_fn(model(inputs), targets)\nprint(\n\t\"Analytical solution: {:.4f}\".format(soln_mse),\n\t\"Numerical solution:  {:.4f}\".format(optim_mse),\n\tsep = \"\\n\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalytical solution: 22.0090\nNumerical solution:  35.7610\n```\n:::\n:::\n\n\nThat's a pretty big difference `r emoji::emoji(\"smile\")`. I guess it goes\nto show that good old-fashioned statistics is often way faster than these\nfancy methods that people seem to like. Like I said, it really worries me that\nthis numerical approach seems to be a lot of tech/DS enthusiast's first intro\nto linear regression, and it's really not very good. But I believe in the\nnext exercises we'll get into some problems that actually need these types of\nmodels. That's all for now!\n\n<!-- END OF FILE -->\n\n",
    "supporting": [
      "Assignment1-BostonHousing_files"
    ],
    "filters": [],
    "includes": {}
  }
}