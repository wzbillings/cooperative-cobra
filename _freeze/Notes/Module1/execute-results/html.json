{
  "hash": "40b879d634ff612f36be953b894071ef",
  "result": {
    "markdown": "---\ntitle: \"Part 1: Tensors and Gradients\"\ndate: last-modified\nauthor: Zane Billings\ntoc: true\nnumber-sections: true\n---\n\nOK, since we're doing Python now, the first thing we have to do is import\nthe packages we want to use (into the namespace, without attaching anything).\nI actually strongly prefer Python's package\ndeclaration system over R's (and I often use the `box` package for R to emulate\nthis behavior).\n\nI was kind of confused that everyone calls this \"PyTorch\" but the library is\njust called `torch`, but I guess that makes sense. Probably `torch` is\nimplemented in some low-level language and there are multiple high-level\ninterfaces to it. Maybe after this I should learn how to use `torch` with `R`.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n# Tensors\n\nApparently, `torch` is a high-performance framework for the manipulation of\ntensors. I didn't know that until today.\n\nThe simplest way to think about a tensor is as a matrix that can have as\nmany different dimensions as you want. I think this doesn't really do them\njustice in the same way that thinking about vectors as lists of numbers and\nmatrices as stacked vectors doesn't tell the whole truth, but tensors are hard\nto think about. Anyways I'll stop rambling and just work through the examples\nfor a bit.\n\nThis code creates a tensor with one floating point element (AKA a zero-order\ntensor or scalar).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nt1 = torch.tensor(4.0)\nt1\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\ntensor(4.)\n```\n:::\n:::\n\n\nWe can check the type of the tensor.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nt1.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\ntorch.float32\n```\n:::\n:::\n\n\nIt's interesting that the default is a 32-bit floating point but I guess\nthat is probably fine. I also noticed that you can make integer-valued\ntensors, but not string-valued tensors.\n\nTensors of higher rank can be specified using Python's list syntax. (I guess\nthis is probably actually creating a list and then type casting it to a\ntensor, but I'm not sure and I haven't looked into it.) Here we can make\nuse of some implicit conversion rules, which I believe are features of\n`torch`. (As in Python, a list can have integers and floats in it at the same\ntime.) Apparently as long as we specify the first number as a float, we don't\nhave to worry about doing that for the rest of the elements.\n\nIt seems that `torch` tensors can only contain one type of element, similar to\na `matrix` in `R` (although those are allowed to contain characters).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Order 1 tensor (AKA vector)\nt2 = torch.tensor([1., 2, 3, 4])\nt2\nt2.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\ntorch.float32\n```\n:::\n:::\n\n\nSpecifying higher dimensional tensors is easily done using a list of lists.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Order 2 tensor (AKA matrix)\nt3 = torch.tensor([[5., 6], [7, 8], [9, 10]])\nt3\n\n# Order 3 tensor (what I would casually call a tensor)\nt4 = torch.tensor([\n\t[[11, 12, 13], [14, 15, 16]],\n\t[[17, 18, 19], [20, 21, 22.]]\n])\nt4\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\ntensor([[[11., 12., 13.],\n         [14., 15., 16.]],\n\n        [[17., 18., 19.],\n         [20., 21., 22.]]])\n```\n:::\n:::\n\n\nNote from this example that any of the elements is allowed to be floating point,\nand the entire tensor will be filled with floats.\n\nLike a matrix, a tensor cannot be \"ragged\" in any dimension. They must be\nconsistent within each dimension -- that is, if one row of a matrix has 7\nelements, the rest of the rows must also have 7 elements. Tensors behave the\nsame way across all of their dimensions.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nt5 = torch.tensor([[5., 6], [7, 8], [9]])\n```\n\n::: {.cell-output .cell-output-error}\n```\nValueError: expected sequence of length 2 at dim 1 (got 1)\n```\n:::\n:::\n\n\nWe can access the `.shape` attribute of a tensor to see the number of dimensions\nand the length in each dimension.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nt1.shape\nt2.shape\nt3.shape\nt4.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\ntorch.Size([2, 2, 3])\n```\n:::\n:::\n\n\nWe can see that the scalar number, `t1`, has no dimensions. Other than that,\nthe dimensions are listed from \"outermost\" to \"innermost\" (if we think about\ntensors as lists of lists of lists ofâ€¦).\n\n# Basic tensor math\n\nStandard arithmetic operations can be used on tensors as well. For example,\nwe can do scalar arithmetic. First we'll initialize the tensors we want.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad = True)\nb = torch.tensor(5., requires_grad = True)\nx; w; b\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\ntensor(5., requires_grad=True)\n```\n:::\n:::\n\n\nNow we can create a new tensor with some math. We'll see what the\n`requires_grad` option does in a moment.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ny = w * x + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\ntensor(17., grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nWe check that\n$$3(4) + 5 = 17$$\nand see no surprises here. However! We are about to see the amazing autodiff\npotential of `torch`. We call the `.backward()` method to find all of the\ncomponents of `y`, and compute the gradient of `y` with respect to those\ncomponents where `requires_grad = True`.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ny.backward()\n\nprint('dy/dx:', x.grad)\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx: None\ndy/dw: tensor(3.)\ndy/db: tensor(1.)\n```\n:::\n:::\n\n\nThe method of accessing these gradients is interesting to me -- I feel like\nthey should be properties of `y` rather than of the constituent parts, and\nI also am not a fan of mutating methods, especially if it mutates an object\nthat I haven't even called explicitly. But I'm not here to complain about OOP\nor the `torch` interface, I'm here to learn something new.\n\nAnyways, we see that the derivatives are what we would expect. We get `None`\nfor the derivative wrt `x` because we didn't set `requires_grad = True` for `x`.\nFor `b` and `w` we get\n$$\\frac{dy}{db} = \\frac{d}{db}\\left(wx + b\\right) = 0 + 1 = 1$$\nand\n$$\\frac{dy}{dw} = \\frac{d}{dw}\\left(wx + b\\right) = x + 0 = 3.$$\n\nSo as much as I don't like the syntax, that's pretty neat.\n\n# Using `numpy` with `torch`\n\nThe `torch` package interoperates with `numpy`, which allows the use of\na lot of the `scipy` ecosystem, like `pandas` and `matplotlib`.\n\nHere's how we can create an array in `numpy`.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np # This is conventional\nx = np.array([[1, 2], [3, 4.]])\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\narray([[1., 2.],\n       [3., 4.]])\n```\n:::\n:::\n\n\n`torch` has a function for an explicit type conversion from `numpy`.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny = torch.from_numpy(x)\ny; x.dtype; y.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\ntorch.float64\n```\n:::\n:::\n\n\nSimilarly, we can convert back to a `numpy` array.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nz = y.numpy\nz\n```\n\n::: {.cell-output .cell-output-display execution_count=40}\n```\n<function Tensor.numpy>\n```\n:::\n:::\n\n\nNow I remember all my frustrations of using python and trying to remember what\nis a method and what is a mutator and what is a function. Functional programming\nreally is so nice and here I am forced back into the OOP world. But I suppose\nI will get used to it over the course of this tutorial.\n\n# Further reading\n\n> 1. What if one or more of $x$, $w$, or $b$ were matrices, instead of\nnumbers in the above example? What would the result $y$ and the gradients\nlook like in this case?\n\nIt's been a while since I took linear algebra and vector calculus, so instead\nof me trying to prove my answer and explain calculus on matrices, what if\nI just type in some stuff and find out instead?\n\nIn this first case, $x$ is a vector, but $w$ and $b$ are scalars.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nx = torch.tensor([1, 2, 3, 4.])\ny = x * w + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\ntensor([ 9., 13., 17., 21.], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nSo in this case, the operations were \"broadcasted\" over the vector $x$. That is,\neach element of $x$ was multiplied by $w$, and then $b$ was added to each element.\n\nI would bet that if $x$ were a matrix, we would see the same behavior.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nx = torch.tensor([[1, 2], [3, 4.]])\ny = x * w + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\ntensor([[ 9., 13.],\n        [17., 21.]], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nYes, the same thing. That's not surprising, it's the most common way to define\nthese arithmetic operations between a higher-order tensor and a scalar. I\nusually call this \"elementwise\" behavior but there are other names.\n\nSo what if we make $b$ a matrix?\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nb = torch.tensor([[1, 2], [3, 4.]])\ny = x * w + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\ntensor([[ 5., 10.],\n        [15., 20.]], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nWe get regular matrix addition, that's good. I assume that $b$ has to conform\nin dimension or we'll get an error.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nb = torch.tensor([1, 2])\ny = x * w + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntensor([[ 5., 10.],\n        [13., 18.]], grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nHmm, no we don't. Recall that\n$$x * w = \\begin{bmatrix}4 & 8 \\\\ 12 & 16\\end{bmatrix},$$\nso it looks like $b$ was added to the first row and to the second row. That's\nan interesting, and IMO understandard, broadcasting behavior. `R` sort of\nbehaves like this in some circumstances with vector recycling, but I would\nstrongly prefer to get an error about matrix conformation here. I can't think\nof a circumstance when I'd want to do this where I wouldn't want to explicitly\nturn $b$ into a matrix of the correct form.\n\nNow maybe the most interesting test: what happens when $w$ is a vector or matrix?\nFor this part, I'll ignore $b$ and just do some tests.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nw = torch.tensor([2, 3])\nx * w\n```\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\ntensor([[ 2.,  6.],\n        [ 6., 12.]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nw = torch.tensor([[2, 3], [4, 5]])\nx * w\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\ntensor([[ 2.,  6.],\n        [12., 20.]])\n```\n:::\n:::\n\n\nOk, so in both of these cases we see the same behavior. There is no attempt\nto do a matrix or tensor product, so I assume we have special commands for\nthose. Instead we get the elementwise product with the same repeating\nbehavior that we saw before. What if we do something with really different\ndimensions?\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nw = torch.tensor([[2, 3, 4], [4, 5, 6], [6, 7, 8]])\nx * w\n```\n\n::: {.cell-output .cell-output-error}\n```\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n```\n:::\n:::\n\n\nOk, good, that doesn't work at all. I was worried part of $w$ would just get\ncut off or something. What if only one of the dimensions is not-repeatable?\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nw = torch.tensor([[2, 3, 4], [4, 5, 6]])\nx * w\n```\n\n::: {.cell-output .cell-output-error}\n```\nRuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1\n```\n:::\n:::\n\n\nAlso an error, that's good.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\nw = torch.tensor([[2, 3], [4, 5], [6, 7], [8, 9]])\nx * w\n```\n\n::: {.cell-output .cell-output-error}\n```\nRuntimeError: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0\n```\n:::\n:::\n\n\nOk interestingly, that time, $x$ doesn't get repeated. So I wonder if it\nwill only repeat the second tensor?\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nw = x\nx = torch.tensor([2, 3])\nx * w\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\ntensor([[ 2.,  6.],\n        [ 6., 12.]])\n```\n:::\n:::\n\n\nNope, it will repeat the first one also, just only in some certain circumstances.\n\nOk, I could keep messing with this forever, but I should probably just read\nthe documentation about pytorch tensor math so I don't trick myself. Let's\nmove on.\n\n> 2. What if $y$ were a matrix, with each element of the matrix expressed\nas a combination of $x$, $w$, and $b$?\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad = True)\nb = torch.tensor(5., requires_grad = True)\ny = torch.tensor([w * x + b, b * x + w])\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\ntensor([17., 19.])\n```\n:::\n:::\n\n\nOk, let's test this, then I'll test a $2\\times 2$ version of $y$. Hopefully\n`torch` will just use the regular rules of matrix differentiation.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ny.backward()\n\nprint('dy/dx:', x.grad)\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)\n```\n\n::: {.cell-output .cell-output-error}\n```\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n```\n:::\n:::\n\n\nWell, I have absolutely no idea what is going on here. Do we get the\nsame nothing for a matrix $y$?\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ny = torch.tensor(\n\t[[w * x + b, b * x + w], [w ** 2 * x, b ** 2 * x]],\n\trequires_grad = True\n\t)\ny.backward()\n```\n\n::: {.cell-output .cell-output-error}\n```\nRuntimeError: grad can be implicitly created only for scalar outputs\n```\n:::\n:::\n\n\nYes, I get the same runtime error that something is not working right. From the\nerror, I assume that this method won't actually let us calculate the derivative\nof a matrix. Maybe I will learn more about that in this tutorial.\n\n> 3. What if we had a chain of operations instead of just one? I.e.\n$y = x * w + b; \\quad z = l * y + m; \\quad w = c * z + d$? What would calling\n`w.grad` do?\n\nI assume that we can use the chain rule to get these gradients (since I\nassume backward means we are doing backpropagation, AKA, the chain rule).\nLet's see if it works for the scalar tensors.\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nc = torch.tensor(2., requires_grad = True)\nz = torch.tensor(4., requires_grad = True)\nd = torch.tensor(1./3., requires_grad = True)\nx = torch.tensor(3.)\nw = c * z + d\nb = torch.tensor(5., requires_grad = True)\ny = w * x + b\nl = torch.tensor(0.5, requires_grad = True)\nm = torch.tensor(0.3, requires_grad = True)\nz = l * y + m\nz.backward()\nprint(\"dz/dl:\", l.grad)\nprint(\"dz/dm:\", m.grad)\nprint(\"dz/dy:\", y.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndz/dl: tensor(30.)\ndz/dm: tensor(1.)\ndz/dy: None\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\Zane\\AppData\\Local\\Temp\\ipykernel_12152\\3256527364.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:485.)\n  print(\"dz/dy:\", y.grad)\n```\n:::\n:::\n\n\nSo we can get the derivatives for $l$ and $m$, but not for $z$. I truly have\nno idea what the rules of what I'm allowed to autodiff or not are, and the\ndocumentation is not very understandable for beginners. So maybe I'll have\nto find another tutorial to read.\n\n<!-- END OF FILE -->\n\n",
    "supporting": [
      "Module1_files"
    ],
    "filters": [],
    "includes": {}
  }
}