{
  "hash": "8714295eee3a94102581fae4cdc48f14",
  "result": {
    "markdown": "---\ntitle: \"Part 1: Tensors and Gradients\"\ndate: last-modified\nauthor: Zane Billings\ntoc: true\nnumber-sections: true\n---\n\nOK, since we're doing Python now, the first thing we have to do is import\nthe packages we want to use (into the namespace, without attaching anything).\nI actually strongly prefer Python's package\ndeclaration system over R's (and I often use the `box` package for R to emulate\nthis behavior).\n\nI was kind of confused that everyone calls this \"PyTorch\" but the library is\njust called `torch`, but I guess that makes sense. Probably `torch` is\nimplemented in some low-level language and there are multiple high-level\ninterfaces to it. Maybe after this I should learn how to use `torch` with `R`.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n```\n:::\n\n\n# Tensors\n\nApparently, `torch` is a high-performance framework for the manipulation of\ntensors. I didn't know that until today.\n\nThe simplest way to think about a tensor is as a matrix that can have as\nmany different dimensions as you want. I think this doesn't really do them\njustice in the same way that thinking about vectors as lists of numbers and\nmatrices as stacked vectors doesn't tell the whole truth, but tensors are hard\nto think about. Anyways I'll stop rambling and just work through the examples\nfor a bit.\n\nThis code creates a tensor with one floating point element (AKA a zero-order\ntensor or scalar).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nt1 = torch.tensor(4.0)\nt1\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\ntensor(4.)\n```\n:::\n:::\n\n\nWe can check the type of the tensor.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nt1.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\ntorch.float32\n```\n:::\n:::\n\n\nIt's interesting that the default is a 32-bit floating point but I guess\nthat is probably fine. I also noticed that you can make integer-valued\ntensors, but not string-valued tensors.\n\nTensors of higher rank can be specified using Python's list syntax. (I guess\nthis is probably actually creating a list and then type casting it to a\ntensor, but I'm not sure and I haven't looked into it.) Here we can make\nuse of some implicit conversion rules, which I believe are features of\n`torch`. (As in Python, a list can have integers and floats in it at the same\ntime.) Apparently as long as we specify the first number as a float, we don't\nhave to worry about doing that for the rest of the elements.\n\nIt seems that `torch` tensors can only contain one type of element, similar to\na `matrix` in `R` (although those are allowed to contain characters).\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Order 1 tensor (AKA vector)\nt2 = torch.tensor([1., 2, 3, 4])\nt2\nt2.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\ntorch.float32\n```\n:::\n:::\n\n\nSpecifying higher dimensional tensors is easily done using a list of lists.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Order 2 tensor (AKA matrix)\nt3 = torch.tensor([[5., 6], [7, 8], [9, 10]])\nt3\n\n# Order 3 tensor (what I would casually call a tensor)\nt4 = torch.tensor([\n\t[[11, 12, 13], [14, 15, 16]],\n\t[[17, 18, 19], [20, 21, 22.]]\n])\nt4\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\ntensor([[[11., 12., 13.],\n         [14., 15., 16.]],\n\n        [[17., 18., 19.],\n         [20., 21., 22.]]])\n```\n:::\n:::\n\n\nNote from this example that any of the elements is allowed to be floating point,\nand the entire tensor will be filled with floats.\n\nLike a matrix, a tensor cannot be \"ragged\" in any dimension. They must be\nconsistent within each dimension -- that is, if one row of a matrix has 7\nelements, the rest of the rows must also have 7 elements. Tensors behave the\nsame way across all of their dimensions.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nt5 = torch.tensor([[5., 6], [7, 8], [9]])\n```\n\n::: {.cell-output .cell-output-error}\n```\nValueError: expected sequence of length 2 at dim 1 (got 1)\n```\n:::\n:::\n\n\nWe can access the `.shape` attribute of a tensor to see the number of dimensions\nand the length in each dimension.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nt1.shape\nt2.shape\nt3.shape\nt4.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\ntorch.Size([2, 2, 3])\n```\n:::\n:::\n\n\nWe can see that the scalar number, `t1`, has no dimensions. Other than that,\nthe dimensions are listed from \"outermost\" to \"innermost\" (if we think about\ntensors as lists of lists of lists ofâ€¦).\n\n# Basic tensor math\n\nStandard arithmetic operations can be used on tensors as well. For example,\nwe can do scalar arithmetic. First we'll initialize the tensors we want.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad = True)\nb = torch.tensor(5., requires_grad = True)\nx; w; b\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\ntensor(5., requires_grad=True)\n```\n:::\n:::\n\n\nNow we can create a new tensor with some math. We'll see what the\n`requires_grad` option does in a moment.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ny = w * x + b\ny\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\ntensor(17., grad_fn=<AddBackward0>)\n```\n:::\n:::\n\n\nWe check that\n$$3(4) + 5 = 17$$\nand see no surprises here. However! We are about to see the amazing autodiff\npotential of `torch`. We call the `.backward()` method to find all of the\ncomponents of `y`, and compute the gradient of `y` with respect to those\ncomponents where `requires_grad = True`.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\ny.backward()\n\nprint('dy/dx:', x.grad)\nprint('dy/dw:', w.grad)\nprint('dy/db:', b.grad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndy/dx: None\ndy/dw: tensor(3.)\ndy/db: tensor(1.)\n```\n:::\n:::\n\n\nThe method of accessing these gradients is interesting to me -- I feel like\nthey should be properties of `y` rather than of the constituent parts, and\nI also am not a fan of mutating methods, especially if it mutates an object\nthat I haven't even called explicitly. But I'm not here to complain about OOP\nor the `torch` interface, I'm here to learn something new.\n\nAnyways, we see that the derivatives are what we would expect. We get `None`\nfor the derivative wrt `x` because we didn't set `requires_grad = True` for `x`.\nFor `b` and `w` we get\n$$\\frac{dy}{db} = \\frac{d}{db}\\left(wx + b\\right) = 0 + 1 = 1$$\nand\n$$\\frac{dy}{dw} = \\frac{d}{dw}\\left(wx + b\\right) = x + 0 = 3.$$\n\nSo as much as I don't like the syntax, that's pretty neat.\n\n# Using `numpy` with `torch`\n\nThe `torch` package interoperates with `numpy`, which allows the use of\na lot of the `scipy` ecosystem, like `pandas` and `matplotlib`.\n\nHere's how we can create an array in `numpy`.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport numpy as np # This is conventional\nx = np.array([[1, 2], [3, 4.]])\nx\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\narray([[1., 2.],\n       [3., 4.]])\n```\n:::\n:::\n\n\n`torch` has a function for an explicit type conversion from `numpy`.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny = torch.from_numpy(x)\ny; x.dtype; y.dtype\n```\n\n::: {.cell-output .cell-output-display execution_count=51}\n```\ntorch.float64\n```\n:::\n:::\n\n\nSimilarly, we can convert back to a `numpy` array.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nz = y.numpy\nz\n```\n\n::: {.cell-output .cell-output-display execution_count=52}\n```\n<function Tensor.numpy>\n```\n:::\n:::\n\n\nNow I remember all my frustrations of using python and trying to remember what\nis a method and what is a mutator and what is a function. Functional programming\nreally is so nice and here I am forced back into the OOP world. But I suppose\nI will get used to it over the course of this tutorial.\n\n<!-- END OF FILE -->\n\n",
    "supporting": [
      "Module1_files"
    ],
    "filters": [],
    "includes": {}
  }
}